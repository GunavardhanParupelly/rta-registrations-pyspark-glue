dashboardh link : https://analytics.zoho.in/open-view/497352000000020225


# ğŸš— Vehicle Registration ETL & Data Warehouse (AWS Glue + PySpark)

## ğŸ“˜ Project Overview
This project demonstrates a **production-grade ETL pipeline** built using **AWS Glue and PySpark**, designed to ingest, clean, and model vehicle registration data into a **star schema** optimized for analytics in **Amazon Redshift or Athena**.

The pipeline follows an industry-standard **Bronze â†’ Silver â†’ Gold** architecture:
- **ETL1** performs extensive data cleaning and staging.
- **ETL2** applies advanced data modeling, fuzzy matching, and dimensionâ€“fact construction.
- The **final DWH schema** supports scalable analytical queries and reporting dashboards.

---

## ğŸ§© Repository Structure

| File | Purpose |
|------|----------|
| `etl1_clean_and_stage.py` | **ETL1 (Raw â†’ Stage):** Cleans raw vehicle registration data, fixes schema drift, normalizes fields (dates, maker, model, fuel), deduplicates by registration number, and writes clean Parquet files to S3. |
| `etl2_advclean_and_dimcreation.py` | **ETL2 (Stage â†’ Gold):** Builds **dimension and fact tables** with surrogate key generation, fuzzy vehicle resolution, emission standard derivation, and adaptive file coalescing. Outputs a ready-to-load star schema layer. |
| `starschema.txt` | **Data Warehouse DDL:** SQL schema for the analytical layer, including dimensions (`vehicle`, `manufacturer`, `rta`, `date`) and the `fact_registrations` fact table with relational integrity. |

---

## ğŸ—ï¸ End-to-End Architecture

```text
Raw CSVs in S3
     â”‚
     â”œâ”€â”€â–¶ ETL1: etl1_clean_and_stage.py
     â”‚       - Cleans malformed data
     â”‚       - Parses inconsistent date formats
     â”‚       - Derives model, variant, and make year
     â”‚       - Deduplicates and partitions by year/month
     â”‚       - Outputs to: s3://.../stage_clean_source/
     â”‚
     â””â”€â”€â–¶ ETL2: etl2_advclean_and_dimcreation.py
             - Builds dim_vehicle, dim_manufacturer, dim_rta
             - Generates surrogate keys using SHA2 hashing
             - Resolves fuzzy duplicates (Levenshtein distance)
             - Creates fact_registrations table
             - Dynamically coalesces files for Redshift efficiency
             - Outputs to: s3://.../gold_dim_* and /gold_fact_registrations/
